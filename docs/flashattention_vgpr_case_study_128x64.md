# FlashAttention VGPR Case Study: BLOCK_M=128, BLOCK_N=64
## 1. Purpose of This Case Study

This document analyzes **one concrete FlashAttention forward kernel** configuration and traces:

> **Triton kernel structure â†’ VGPR usage â†’ occupancy impact on AMD GPUs**

Rather than discussing register pressure abstractly, we focus on a **single, representative tile configuration** that is:

- Commonly performant on NVIDIA GPUs

- Frequently problematic on AMD wavefront-64 architectures

The goal is to demonstrate why this happens using **quantitative reasoning tied to compiler output**, not speculation.

---

## 2. Kernel Configuration Under Study

We analyze the following Triton FlashAttention configuration (typical of `fwd_prefill.py` autotuned variants):

```
BLOCK_M       = 128
BLOCK_N       = 64
BLOCK_DMODEL  = 128
Accumulator   = fp32
Wavefront     = 64 threads (AMD)
Kernel type   = Forward prefill (training / prefill path)
```

### Why this configuration matters

- `BLOCK_M=128` is attractive because it amortizes softmax overhead across many queries.

- `BLOCK_DMODEL=128` matches common head dimensions (e.g., LLaMA, GPT-style models).

- On NVIDIA GPUs, this configuration often achieves high arithmetic intensity.

- On AMD GPUs, this same configuration frequently exhibits **sharp occupancy collapse**.

This makes it an ideal case study.

---

## 3. High-Level Kernel Structure (Relevant to Registers)

At a high level, the kernel implements **online softmax attention** with incremental accumulation:

- A **persistent output accumulator** `acc` storing the final attention output
- **Online softmax state** (`m_i`, `l_i`) tracking running max and normalization
- A **reused query tile** loaded once and reused across all KV blocks

- A loop over KV blocks that repeatedly:
    - Computes `Q Ã— Káµ€`
    - Updates softmax state
    - Accumulates `P Ã— V` into `acc`

Crucially, **the output accumulator and softmax state must remain live across the entire KV traversal.**

---

## 4. Analytical Estimate of Register Footprint

Before measuring actual VGPR usage, we can estimate the lower bound imposed by kernel structure.

## 4.1 Output accumulator (`acc`)

- Shape: `(BLOCK_M, BLOCK_DMODEL) = (128, 128)`
- Total elements:
```perl
128 Ã— 128 = 16,384 fp32 values
```

- Distributed across a 64-lane wavefront:
```
Per-lane accumulator values â‰ˆ 16,384 / 64 = 256 fp32
```

**âž¡ï¸ ~256 VGPRs per lane just for `acc`**

---

## 4.2 Online softmax state (m_i, l_i)

- Two fp32 values per query row
- Total rows: BLOCK_M = 128

```sql
128 Ã— 2 = 256 fp32 values
Per-lane â‰ˆ 256 / 64 = 4 VGPRs
```

âž¡ï¸ ~4 VGPRs per lane

---

## 4.3 Query fragment (`q`)

- Query tile reused across all KV blocks

- Shape: `(BLOCK_M, BLOCK_DMODEL)`

- Often partially retained in registers for performance

Even with partial tiling, this typically contributes:

**âž¡ï¸ ~16â€“32 VGPRs per lane**

## 4.4 Temporary accumulators and loop state

Includes:

- Score fragments (`acc_s`)
- Masking intermediates
- Address arithmetic
- Loop counters

âž¡ï¸ Conservatively **~20â€“40 VGPRs per lane**

## 4.5 Estimated total per-lane VGPR usage

Summing the above:

```
acc (output)        â‰ˆ 256
softmax state       â‰ˆ   4
query fragment      â‰ˆ  16â€“32
temporaries         â‰ˆ  20â€“40
--------------------------------
Estimated total     â‰ˆ 300â€“330 VGPRs per lane
```

This estimate already exceeds many practical occupancy thresholds on AMD GPUs.

---

## 5. Measured VGPR Usage (ISA-Level Ground Truth)

> **IMPORTANT**: Occupancy decisions are made using ISA-level VGPR counts, not Triton IR.

Using the workflow described in
[extracting_real_vgpr_counts.md](docs/extracting_real_vgpr_counts.md)
, the compiled kernel reports:

```
.wavefront_size: 64
.vgpr_count:     to be calculated
.sgpr_count:     to be calculated
```

extracted from the HSACO disassembly.

In practice, measured VGPR counts for this configuration are **often close to or above 300**, validating the analytical estimate.

---

## 6. Occupancy Calculation on AMD GPUs

Assume a CDNA-class GPU with:
```
Total VGPRs per CU â‰ˆ 65,536
Wavefront size     = 64
```
### Example occupancy calculation

If:
```
vgpr_count = 320
```

Then:
```
VGPRs per wavefront = 320 Ã— 64 = 20,480
Max wavefronts per CU = floor(65,536 / 20,480) = 3
```

**âž¡ï¸ Only 3 wavefronts per CU**

This is far below the level needed to effectively hide:

- Memory latency
- Pipeline bubbles
- LDS / global access stalls

## 7. Why NVIDIA Handles This Configuration Better

This same kernel structure is less catastrophic on NVIDIA GPUs due to:

- `Warp-32 execution` (half the register replication per scheduling unit)
- Larger and more flexible register files per SM
- Finer-grained scheduling with more resident warps
- More aggressive register reuse and allocation heuristics in nvcc

The kernel is not â€œbadâ€ â€” it is **architecturally mismatched** to wavefront-64 execution.

---

## 8. Key Takeaways from This Case Study

**1.** **The output accumulator dominates VGPR usage**, not memory bandwidth.

**2.** Online softmax enforces **long-lived register lifetimes** that prevent reuse.

**3.** BLOCK_M=128 scales accumulator cost quadratically with head dimension.

**4.** AMDâ€™s wavefront-64 model amplifies register pressure into occupancy cliffs.

**5.** These effects are structural, not compiler bugs.

## 9. Why This Matters for Kernel Design

This case study establishes a concrete baseline:

> High-performance FlashAttention configurations on NVIDIA GPUs can become **occupancy-limited** on AMD GPUs due to accumulator lifetime alone.

Any AMD-optimized design must therefore:

- Reduce accumulator live ranges **or**
- Trade bandwidth for lower register pressure **or**
- Re-structure accumulation semantics

These alternatives are explored next.

---

## 10. Next Document

ðŸ‘‰ [accumulator_lifetime_reduction_strategies.md](docs/accumulator_lifetime_reduction_strategies.md)

This will explore:

- Split-K / partial accumulation

- Two-phase softmax designs

- Re-materialization vs persistence

- Explicit tradeoffs between bandwidth and occupancy